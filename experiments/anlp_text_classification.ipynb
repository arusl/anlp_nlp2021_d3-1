{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4nqEeQIg6Nq"
   },
   "source": [
    "# Source code for ANLP: NLP2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>An Experimental Evaluation of Japanese Tokenizers for Sentiment-Based Text Classification</strong><br/>\n",
    "Andre Rusli & Makoto Shishido (Tokyo Denki University)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qRqZ1MBpc8O"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vfuC4vbeply-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "import MeCab\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train (340K) and Test (40K) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOgFB4FIxDux",
    "outputId": "87a563fb-5529-4d06-e2f7-729838b4751e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples:  340000\n",
      "Total testing samples:  40000\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', header=None)\n",
    "df_test = pd.read_csv('../data/rakuten-sentiment-dataset/binary/sampled_binary_test.csv', header=None)\n",
    "\n",
    "print(\"Total training samples: \", len(df_train))\n",
    "print(\"Total testing samples: \", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "R5uXg-vyyxQJ",
    "outputId": "54572184-a49a-4846-eab7-606ce5901a42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>臭い</td>\n",
       "      <td>余りにも、匂いがきつく安物みたいです。\\n安いから仕方ないかな？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>残念…。</td>\n",
       "      <td>マキシスカートのスリムタイプのグレーを購入したのですが、商品に不備があったとの事でカーキに変...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>発送がメチャクチャ早くて\\nびっくりしました(≧▽≦)\\n包装も、丁寧で厳重で、信頼◎です(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>玄関に飾りました</td>\n",
       "      <td>見た目すっきりですが、クローバーのラインストーンがアクセントになっていていいと思います。\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>うちの子にはダメ</td>\n",
       "      <td>とても良い商品だと思いますが、避妊手術をした推定１才4ヶ月で体重4キロの子には不向きでした。...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1                                                  2\n",
       "0  1        臭い                   余りにも、匂いがきつく安物みたいです。\\n安いから仕方ないかな？\n",
       "1  1      残念…。  マキシスカートのスリムタイプのグレーを購入したのですが、商品に不備があったとの事でカーキに変...\n",
       "2  2       NaN  発送がメチャクチャ早くて\\nびっくりしました(≧▽≦)\\n包装も、丁寧で厳重で、信頼◎です(...\n",
       "3  2  玄関に飾りました  見た目すっきりですが、クローバーのラインストーンがアクセントになっていていいと思います。\\n...\n",
       "4  1  うちの子にはダメ  とても良い商品だと思いますが、避妊手術をした推定１才4ヶ月で体重4キロの子には不向きでした。..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>残念です…</td>\n",
       "      <td>243254-20151129-0858902215\\n生写真プレゼントに惹かれて他の予約商...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1日で毛玉びっしり！</td>\n",
       "      <td>家で1日履いて夜お風呂で脱いだ時には毛玉びっしり！\\n一回きり？もう外では履けない！\\n起毛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>食いつき良し</td>\n",
       "      <td>当方のワンコはドーベルマンです。\\nガムだと７日程度（ＬＬサイズ）、アキレスだとほんの１５分...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>残念</td>\n",
       "      <td>Ｎ１１を購入。\\nでも折り返し部分が幅広だし履いていてチクチク感があり痒い。\\n箪笥の肥やしです。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>使いやすいです</td>\n",
       "      <td>キッチンペーパーがむき出しのホルダーはたくさんあるけれど、キッチンペーパーが見えなくそのうえ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                                                  2\n",
       "0  1       残念です…  243254-20151129-0858902215\\n生写真プレゼントに惹かれて他の予約商...\n",
       "1  1  1日で毛玉びっしり！  家で1日履いて夜お風呂で脱いだ時には毛玉びっしり！\\n一回きり？もう外では履けない！\\n起毛...\n",
       "2  2      食いつき良し  当方のワンコはドーベルマンです。\\nガムだと７日程度（ＬＬサイズ）、アキレスだとほんの１５分...\n",
       "3  1          残念  Ｎ１１を購入。\\nでも折り返し部分が幅広だし履いていてチクチク感があり痒い。\\n箪笥の肥やしです。\n",
       "4  2     使いやすいです  キッチンペーパーがむき出しのホルダーはたくさんあるけれど、キッチンペーパーが見えなくそのうえ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a SentencePiece Model from Train Data (vocab_size=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H83ncfC26keY",
    "outputId": "cd03e987-c657-4887-ca9d-45dc20d3b0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train a SP model w/ 16000 vocab_size, from 340k train data: 361.1047897338867\n",
      "Time to train a SP model w/ 16000 vocab_size, from 340k train data: 360.28389596939087\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-16000-340k', vocab_size=16000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 16000 vocab_size, from 340k train data:', end-start)\n",
    "\n",
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-16000-340k', vocab_size=16000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 16000 vocab_size, from 340k train data:', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train a SP model w/ 16000 vocab_size, from 340k train data: 360.79864263534546\n",
      "Time to train a SP model w/ 8000 vocab_size, from 340k train data: 389.89155197143555\n",
      "Time to train a SP model w/ 8000 vocab_size, from 340k train data: 388.27756357192993\n",
      "Time to train a SP model w/ 8000 vocab_size, from 340k train data: 388.90677785873413\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-16000-340k', vocab_size=16000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 16000 vocab_size, from 340k train data:', end-start)\n",
    "\n",
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-8000-340k', vocab_size=8000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 8000 vocab_size, from 340k train data:', end-start)\n",
    "\n",
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-8000-340k', vocab_size=8000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 8000 vocab_size, from 340k train data:', end-start)\n",
    "\n",
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-8000-340k', vocab_size=8000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 8000 vocab_size, from 340k train data:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train a SP model w/ 32000 vocab_size, from 340k train data: 313.60328674316406\n",
      "Time to train a SP model w/ 32000 vocab_size, from 340k train data: 314.5716471672058\n",
      "Time to train a SP model w/ 32000 vocab_size, from 340k train data: 313.32940673828125\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-32000-340k', vocab_size=32000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 32000 vocab_size, from 340k train data:', end-start)\n",
    "\n",
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-32000-340k', vocab_size=32000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 32000 vocab_size, from 340k train data:', end-start)\n",
    "\n",
    "start = time.time()\n",
    "spm.SentencePieceTrainer.train(input='../data/rakuten-sentiment-dataset/binary/sampled_binary_train.csv', model_prefix='sp-model-32000-340k', vocab_size=32000)\n",
    "end = time.time()\n",
    "print('Time to train a SP model w/ 32000 vocab_size, from 340k train data:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1:\n",
      " ギターのノブにするため買いました。\\n特に悪い所もなく、届いたとき商品はプチプチで包まれていました。\\nどちらかと言うと良かったと思います。\\nまだつけて間もないのでこれからどうなるかわからないですけど 今のところ不自由は何一つないです。\n",
      "MeCab:\n",
      " ['ギター', 'の', 'ノブ', 'に', 'する', 'ため', '買い', 'まし', 'た', '。', '\\\\', 'n', '特に', '悪い', '所', 'も', 'なく', '、', '届い', 'た', 'とき', '商品', 'は', 'プチプチ', 'で', '包ま', 'れ', 'て', 'い', 'まし', 'た', '。', '\\\\', 'n', 'どちら', 'か', 'と', '言う', 'と', '良かっ', 'た', 'と', '思い', 'ます', '。', '\\\\', 'n', 'まだ', 'つけ', 'て', '間', 'も', 'ない', 'の', 'で', 'これ', 'から', 'どう', 'なる', 'か', 'わから', 'ない', 'です', 'けど', '今', 'の', 'ところ', '不', '自由', 'は', '何', '一', 'つ', 'ない', 'です', '。']\n",
      "Sudachi:\n",
      " ['ギター', 'の', 'ノブ', 'に', 'する', 'ため', '買い', 'まし', 'た', '。', '\\\\', 'n', '特に', '悪い', '所', 'も', 'なく', '、', '届い', 'た', 'とき', '商品', 'は', 'プチプチ', 'で', '包ま', 'れ', 'て', 'い', 'まし', 'た', '。', '\\\\', 'n', 'どちら', 'か', 'と', '言う', 'と', '良かっ', 'た', 'と', '思い', 'ます', '。', '\\\\', 'n', 'まだ', 'つけ', 'て', '間', 'も', 'ない', 'の', 'で', 'これ', 'から', 'どう', 'なる', 'か', 'わから', 'ない', 'です', 'けど', ' ', '今', 'の', 'ところ', '不自由', 'は', '何', '一', 'つ', 'ない', 'です', '。']\n",
      "SP:\n",
      " ['▁', 'ギター', 'の', 'ノ', 'ブ', 'に', 'するため', '買いました', '。\\\\', 'n', '特に', '悪い', '所', 'もなく', '、', '届いたとき', '商品は', 'プチプチ', 'で', '包まれて', 'いました', '。\\\\', 'n', 'どちらかと言うと', '良かったと思います', '。\\\\', 'n', 'まだ', 'つけて', '間もないので', 'これから', 'どうなるか', 'わからない', 'ですけど', '▁', '今のところ', '不自由', 'は何', '一つ', 'ないです', '。']\n",
      "\n",
      "\n",
      "Review 2:\n",
      " 自転車通勤用に購入。サックスを選びましたが、きれいな色で、雨でも気分が上がりそうです。\n",
      "MeCab:\n",
      " ['自転', '車', '通勤', '用', 'に', '購入', '。', 'サックス', 'を', '選び', 'まし', 'た', 'が', '、', 'きれい', 'な', '色', 'で', '、', '雨', 'で', 'も', '気分', 'が', '上がり', 'そう', 'です', '。']\n",
      "Sudachi:\n",
      " ['自転車', '通勤用', 'に', '購入', '。', 'サックス', 'を', '選び', 'まし', 'た', 'が', '、', 'きれい', 'な', '色', 'で', '、', '雨', 'で', 'も', '気分', 'が', '上がり', 'そう', 'です', '。']\n",
      "SP:\n",
      " ['▁', '自転車通勤', '用に購入', '。', 'サックス', 'を選びましたが', '、', 'きれいな色で', '、', '雨', 'でも', '気分が', '上がり', 'そうです', '。']\n",
      "\n",
      "\n",
      "Review 3:\n",
      " かわいいです(*^。^*)\\nパソコンで見る色よりすこし濃い感じでした\\nワンショルダーはやっぱり便利ですね\n",
      "MeCab:\n",
      " ['かわいい', 'です', '(', '*^。^*)\\\\', 'n', 'パソコン', 'で', '見る', '色', 'より', 'すこし', '濃い', '感じ', 'でし', 'た', '\\\\', 'n', 'ワン', 'ショルダー', 'は', 'やっぱり', '便利', 'です', 'ね']\n",
      "Sudachi:\n",
      " ['かわいい', 'です', '(*^。^*)', '\\\\', 'n', 'パソコン', 'で', '見る', '色', 'より', 'すこし', '濃い', '感じ', 'でし', 'た', '\\\\', 'n', 'ワンショルダー', 'は', 'やっぱり', '便利', 'です', 'ね']\n",
      "SP:\n",
      " ['▁', 'かわいいです', '(*^。^*)', '\\\\', 'n', 'パソコン', 'で', '見る', '色', 'より', 'すこし', '濃い', '感じでした', '\\\\', 'n', 'ワン', 'ショルダー', 'はやっぱり', '便利ですね']\n"
     ]
    }
   ],
   "source": [
    "# mecab w/ unidic-lite\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "#sudachi\n",
    "sudachi = dictionary.Dictionary().create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.B\n",
    "\n",
    "#sentencepiece\n",
    "sp = spm.SentencePieceProcessor(model_file='./sp-model-32000-340k.model')\n",
    "\n",
    "rev1 = df_train[2][291809]\n",
    "rev2 = df_test[2][17827]\n",
    "rev3 = df_train[2][558]\n",
    "\n",
    "print(\"Review 1:\\n\", rev1)\n",
    "print(\"MeCab:\\n\", wakati.parse(rev1).split())\n",
    "print(\"Sudachi:\\n\", [m.surface() for m in sudachi.tokenize(rev1, mode)])\n",
    "print(\"SP:\\n\", sp.encode(rev1, out_type=str))\n",
    "\n",
    "print(\"\\n\\nReview 2:\\n\", rev2)\n",
    "print(\"MeCab:\\n\", wakati.parse(rev2).split())\n",
    "print(\"Sudachi:\\n\", [m.surface() for m in sudachi.tokenize(rev2, mode)])\n",
    "print(\"SP:\\n\", sp.encode(rev2, out_type=str))\n",
    "\n",
    "print(\"\\n\\nReview 3:\\n\", rev3)\n",
    "print(\"MeCab:\\n\", wakati.parse(rev3).split())\n",
    "print(\"Sudachi:\\n\", [m.surface() for m in sudachi.tokenize(rev3, mode)])\n",
    "print(\"SP:\\n\", sp.encode(rev3, out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rLgrHkCxxIAd"
   },
   "outputs": [],
   "source": [
    "def tokenize_sp(text):\n",
    "    tokenized = sp.encode(text, out_type=str)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mecab(text):\n",
    "    tokenized = wakati.parse(text).split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sudachi(text):\n",
    "    tokenized = [m.surface() for m in sudachi.tokenize(text, mode)]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UykJwDFTxM_I",
    "outputId": "96ff419f-b8d7-40e2-b4f5-33b69e5e295a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples:  340000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train[2], df_train[0]\n",
    "print(\"Total train samples: \", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tCQuwRzlxQJz"
   },
   "outputs": [],
   "source": [
    "tfidfVect_mecab = TfidfVectorizer(tokenizer=tokenize_mecab)\n",
    "tfidfVect_sudachi = TfidfVectorizer(tokenizer=tokenize_sudachi)\n",
    "tfidfVect_sp = TfidfVectorizer(tokenizer=tokenize_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RW1baPMR4-WO",
    "outputId": "3d401d68-4bff-41bc-adf0-66382023097e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Vect time (MeCab):  27.098011016845703\n",
      "TFIDF Vect time (Sudachi):  1417.198209285736\n",
      "TFIDF Vect time (SentencePiece):  19.105496168136597\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train_tfidf_mecab = tfidfVect_mecab.fit_transform(X_train)\n",
    "end = time.time()\n",
    "print(\"TFIDF Vect time (MeCab): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "X_train_tfidf_sudachi = tfidfVect_sudachi.fit_transform(X_train)\n",
    "end = time.time()\n",
    "print(\"TFIDF Vect time (Sudachi): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "X_train_tfidf_sp = tfidfVect_sp.fit_transform(X_train)\n",
    "end = time.time()\n",
    "print(\"TFIDF Vect time (SentencePiece): \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEE2VdlIjYY2",
    "outputId": "d5255b49-a537-4c05-d282-3c99235bb477"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrejpn19/ENTER/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (LR-MeCab):  21.328707456588745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrejpn19/ENTER/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (LR-Sudachi):  21.394134998321533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrejpn19/ENTER/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (LR-SP):  13.090460777282715\n",
      "Training time (MNB-MeCab):  0.1601853370666504\n",
      "Training time (MNB-Sudachi):  0.11599230766296387\n",
      "Training time (MNB-SP):  0.0952596664428711\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf_lr_mecab = LogisticRegression(random_state=0).fit(X_train_tfidf_mecab, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (LR-MeCab): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "clf_lr_sudachi = LogisticRegression(random_state=0).fit(X_train_tfidf_sudachi, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (LR-Sudachi): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "clf_lr_sp = LogisticRegression(random_state=0).fit(X_train_tfidf_sp, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (LR-SP): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "clf_mnb_mecab = MultinomialNB().fit(X_train_tfidf_mecab, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (MNB-MeCab): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "clf_mnb_sudachi = MultinomialNB().fit(X_train_tfidf_sudachi, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (MNB-Sudachi): \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "clf_mnb_sp = MultinomialNB().fit(X_train_tfidf_sp, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (MNB-SP): \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Error Rate = 1 - accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples:  340000\n",
      "Train Error rate (LR-MeCab) 8.514999999999995\n",
      "Train Error rate (LR-Sudachi) 8.456764705882359\n",
      "Train Error rate (LR-SP) 6.476176470588236\n",
      "Train Error rate (MNB-MeCab) 11.237352941176471\n",
      "Train Error rate (MNB-Sudachi) 11.039705882352946\n",
      "Train Error rate (MNB-SP) 8.277941176470593\n"
     ]
    }
   ],
   "source": [
    "# Predictions on train set\n",
    "X_train, y_train = df_train[2], df_train[0]\n",
    "print(\"Total train samples: \", len(X_train))\n",
    "\n",
    "predicted = clf_lr_mecab.predict(X_train_tfidf_mecab)\n",
    "print(\"Train Error rate (LR-MeCab)\", (1-np.mean(predicted == y_train))*100)\n",
    "\n",
    "predicted = clf_lr_sudachi.predict(X_train_tfidf_sudachi)\n",
    "print(\"Train Error rate (LR-Sudachi)\", (1-np.mean(predicted == y_train))*100)\n",
    "\n",
    "predicted = clf_lr_sp.predict(X_train_tfidf_sp)\n",
    "print(\"Train Error rate (LR-SP)\", (1-np.mean(predicted == y_train))*100)\n",
    "\n",
    "predicted = clf_mnb_mecab.predict(X_train_tfidf_mecab)\n",
    "print(\"Train Error rate (MNB-MeCab)\", (1-np.mean(predicted == y_train))*100)\n",
    "\n",
    "predicted = clf_mnb_sudachi.predict(X_train_tfidf_sudachi)\n",
    "print(\"Train Error rate (MNB-Sudachi)\", (1-np.mean(predicted == y_train))*100)\n",
    "\n",
    "predicted = clf_mnb_sp.predict(X_train_tfidf_sp)\n",
    "print(\"Train Error rate (MNB-SP)\", (1-np.mean(predicted == y_train))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcqW4CixxWn4",
    "outputId": "1aed4bdd-cb93-4fde-b804-1079bbcd6db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples:  40000\n",
      "Error rate (LR-MeCab) 9.635000000000005\n",
      "Error rate (LR-Sudachi) 9.607500000000002\n",
      "Error rate (LR-SP) 7.997500000000002\n",
      "Error rate (MNB-MeCab) 12.517500000000004\n",
      "Error rate (MNB-Sudachi) 12.417500000000004\n",
      "Error rate (MNB-SP) 8.897500000000003\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "X_test, y_test = df_test[2], df_test[0]\n",
    "print(\"Total test samples: \", len(X_test))\n",
    "\n",
    "X_test_tfidf = tfidfVect_mecab.transform(X_test)\n",
    "predicted = clf_lr_mecab.predict(X_test_tfidf)\n",
    "print(\"Error rate (LR-MeCab)\", (1-np.mean(predicted == y_test))*100)\n",
    "\n",
    "X_test_tfidf = tfidfVect_sudachi.transform(X_test)\n",
    "predicted = clf_lr_sudachi.predict(X_test_tfidf)\n",
    "print(\"Error rate (LR-Sudachi)\", (1-np.mean(predicted == y_test))*100)\n",
    "\n",
    "X_test_tfidf = tfidfVect_sp.transform(X_test)\n",
    "predicted = clf_lr_sp.predict(X_test_tfidf)\n",
    "print(\"Error rate (LR-SP)\", (1-np.mean(predicted == y_test))*100)\n",
    "\n",
    "X_test_tfidf = tfidfVect_mecab.transform(X_test)\n",
    "predicted = clf_mnb_mecab.predict(X_test_tfidf)\n",
    "print(\"Error rate (MNB-MeCab)\", (1-np.mean(predicted == y_test))*100)\n",
    "\n",
    "X_test_tfidf = tfidfVect_sudachi.transform(X_test)\n",
    "predicted = clf_mnb_sudachi.predict(X_test_tfidf)\n",
    "print(\"Error rate (MNB-Sudachi)\", (1-np.mean(predicted == y_test))*100)\n",
    "\n",
    "X_test_tfidf = tfidfVect_sp.transform(X_test)\n",
    "predicted = clf_mnb_sp.predict(X_test_tfidf)\n",
    "print(\"Error rate (MNB-SP)\", (1-np.mean(predicted == y_test))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-vIiICjxXta"
   },
   "outputs": [],
   "source": [
    "# Predictions on few example reviews\n",
    "X_new = ['悪い商品で、値段も高すぎる', '満足！']\n",
    "X_new_tfidf = tfidfVect_mecab.transform(X_new)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV -- Logistics Regression Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.920125 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.907683 (0.001330) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.916786 (0.001765) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.907678 (0.001332) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.918808 (0.001292) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.920125 (0.001375) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.918800 (0.001293) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.919631 (0.001389) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.919613 (0.001409) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.919633 (0.001388) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.896632 (0.001390) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.896648 (0.001387) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.896633 (0.001389) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.843839 (0.001359) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.843845 (0.001352) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.843910 (0.001370) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Training time (LR-SP):  1454.3478364944458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrejpn19/ENTER/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV => Logistics Regression + SentencePiece\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = time.time()\n",
    "# set parameters\n",
    "model = LogisticRegression(random_state=0)\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train_tfidf_sp, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "end = time.time()\n",
    "print(\"Training time (LR-SP): \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics Regression(random_state=0, C=10, penalty=l2, solver lbfgs -- SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrejpn19/ENTER/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (LR-SP):  13.420908451080322  seconds.\n",
      "Train Error rate (LR-SP) 5.560588235294118\n",
      "Total test samples:  40000\n",
      "Test Error rate (LR-SP) 7.779999999999998\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf_lr_sp = LogisticRegression(random_state=0, C=10, penalty='l2', solver='lbfgs').fit(X_train_tfidf_sp, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time (LR-SP): \", end-start, \" seconds.\")\n",
    "\n",
    "predicted = clf_lr_sp.predict(X_train_tfidf_sp)\n",
    "print(\"Train Error rate (LR-SP)\", (1-np.mean(predicted == y_train))*100)\n",
    "\n",
    "X_test, y_test = df_test[2], df_test[0]\n",
    "print(\"Total test samples: \", len(X_test))\n",
    "\n",
    "X_test_tfidf = tfidfVect_sp.transform(X_test)\n",
    "predicted = clf_lr_sp.predict(X_test_tfidf)\n",
    "print(\"Test Error rate (LR-SP)\", (1-np.mean(predicted == y_test))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dEyxSZL_vzrQ"
   ],
   "name": "anlp-text-classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
